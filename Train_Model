import numpy as np
import tensorflow as tf
from tensorflow.keras import layers
import os
import tarfile

class Generator(tf.keras.Model):
    def __init__(self, latent_dim, num_tracks, num_timesteps, num_features):
        super(Generator, self).__init__()
        self.latent_dim = latent_dim
        self.num_tracks = num_tracks
        self.num_timesteps = num_timesteps
        self.num_features = num_features
        self.dense = layers.Dense(128, input_shape=(latent_dim,))
        self.reshape = layers.Reshape((4, 4, 8))
        self.conv1 = layers.Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same')
        self.conv2 = layers.Conv2DTranspose(1, (4, 4), strides=(2, 2), padding='same')

    def call(self, inputs):
        x = self.dense(inputs)
        x = self.reshape(x)
        x = self.conv1(x)
        x = self.conv2(x)
        return x

class Discriminator(tf.keras.Model):
    def __init__(self, num_tracks, num_timesteps, num_features):
        super(Discriminator, self).__init__()
        self.num_tracks = num_tracks
        self.num_timesteps = num_timesteps
        self.num_features = num_features
        self.conv1 = layers.Conv2D(64, (4, 4), strides=(2, 2), padding='same')
        self.conv2 = layers.Conv2D(128, (4, 4), strides=(2, 2), padding='same')
        self.flatten = layers.Flatten()
        self.dense = layers.Dense(1)

    def call(self, inputs):
        x = self.conv1(inputs)
        x = self.conv2(x)
        x = self.flatten(x)
        x = self.dense(x)
        return x

# Load MIDI data from npz files
def load_data_from_npz(file_paths):
    data = []
    for file_path in file_paths:
        npzfile = np.load(file_path)
        keys = list(npzfile.keys())
        # Assuming the npz files contain a key 'data' with MIDI data
        data.append(npzfile[keys[0]])
    return data

# Preprocess MIDI data (placeholder)
def preprocess_data(data):
    return data

# Extract files from tar.gz archive
def extract_tar_gz(archive_path, target_dir):
    with tarfile.open(archive_path, 'r:gz') as tar:
        tar.extractall(path=target_dir)

# Define hyperparameters
latent_dim = 100
num_tracks = 4
num_timesteps = 64
num_features = 128
batch_size = 64
epochs = 50
learning_rate = 0.0002

# Instantiate models
generator = Generator(latent_dim, num_tracks, num_timesteps, num_features)
discriminator = Discriminator(num_tracks, num_timesteps, num_features)



# Define optimizers and loss function
generator_optimizer = tf.keras.optimizers.Adam(learning_rate)
discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate)
cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)

# Define training step function
@tf.function
def train_step(real_sequences):
    noise = tf.random.normal([batch_size, latent_dim])

    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        generated_sequences = generator(noise, training=True)

        real_output = discriminator(real_sequences, training=True)
        fake_output = discriminator(generated_sequences, training=True)

        gen_loss = generator_loss(fake_output)
        disc_loss = discriminator_loss(real_output, fake_output)

    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))

# Define loss functions
def generator_loss(fake_output):
    return cross_entropy(tf.ones_like(fake_output), fake_output)

def discriminator_loss(real_output, fake_output):
    real_loss = cross_entropy(tf.ones_like(real_output), real_output)
    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
    total_loss = real_loss + fake_loss
    return total_loss

# Load and preprocess data
archive_path = 'lpd_cleansed.tar.gz'
extract_dir = 'путь_к_каталогу_для_извлеченных_файлов'
npz_file_paths = []

# Extract files from the tar.gz archive
extract_tar_gz(archive_path, extract_dir)

# Find npz files in the extracted data
for root, dirs, files in os.walk(extract_dir):
    for file in files:
        if file.endswith('.npz'):
            npz_file_paths.append(os.path.join(root, file))

# Load data from npz files
data = load_data_from_npz(npz_file_paths)

# Preprocess data
processed_data = preprocess_data(data)


# Train the model
for epoch in range(epochs):
    for batch in range(len(processed_data) // batch_size):
        real_sequences = processed_data[batch * batch_size: (batch + 1) * batch_size]
        train_step(real_sequences)

    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss}')
